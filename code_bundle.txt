`# Project Structure

├── .env
├── .gitignore
├── README.md
├── code_bundle.txt
├── config.py
├── db
│   ├── initialize.py
│   ├── models.py
│   ├── repository.py
│   └── session.py
├── key
│   └── pjt-dev-hdegis-app-454401-bd4fac2d452b.json
├── main.py
├── nohup-main.out
├── processor
│   ├── elastic.py
│   ├── embedder.py
│   ├── extractor.py
│   ├── pdf_manager.py
│   └── prompts.py
├── requirements.txt
├── sample.png
├── scheduler
│   ├── orchestator_parallel.py
│   └── orchestrator.py
├── storage
│   └── gcs_client.py
├── test_ieee.pdf
└── utils
    ├── logger.py
    └── utils.py

------------------

## main.py

from scheduler.orchestrator import run_pipeline

def main() -> None:
    run_pipeline()

if __name__ == "__main__":
    main()

------------------

## config.py

from dotenv import load_dotenv
import os

load_dotenv()

# GCP
GOOGLE_APPLICATION_CREDENTIALS: str = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
PROJECT_ID: str = os.getenv("PROJECT_ID")

GCS_SOURCE_BUCKET: str = os.getenv("GCS_SOURCE_BUCKET")
GCS_PROCESSED_BUCKET: str = os.getenv("GCS_PROCESSED_BUCKET")

GENAI_LOCATION: str = os.getenv("GENAI_LOCATION")
EXTRACT_TEXT_MODEL: str = os.getenv("EXTRACT_TEXT_MODEL")
EXTRACT_SUMMARY_MODEL: str = os.getenv("EXTRACT_SUMMARY_MODEL")
EMBEDDING_MODEL: str = os.getenv("EMBEDDING_MODEL")



# MySQL
MYSQL_HOST: str = os.getenv("MYSQL_HOST")
MYSQL_PORT: int = int(os.getenv("MYSQL_PORT"))
MYSQL_USER: str = os.getenv("MYSQL_USER")
MYSQL_PWD: str = os.getenv("MYSQL_PWD")
MYSQL_CHARSET: str = os.getenv("MYSQL_CHARSET")
MYSQL_DB: str = os.getenv("MYSQL_DB")

TABLENAME_PDFPAGE: str = os.getenv("TABLENAME_PDFPAGE")
TABLENAME_PDFDOCUMENT: str = os.getenv("TABLENAME_PDFDOCUMENT")



# Elastic
ES_HOST: str = os.getenv("ES_HOST")
ES_USER: str = os.getenv("ES_USER")
ES_PWD: str = os.getenv("ES_PWD")
CA_CERT: str = os.getenv("CA_CERT")

INDEX_NAME: str = f"hdegis-{EMBEDDING_MODEL}"

# LOG
LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO")

------------------

## processor/elastic.py

import os
import sys
from typing import List, Dict, Tuple, Optional, Any
import warnings
from urllib3.exceptions import InsecureRequestWarning

# InsecureRequestWarning 경고 무시
warnings.simplefilter("ignore", InsecureRequestWarning)

from elasticsearch import Elasticsearch

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from utils.logger import get_logger
from config import LOG_LEVEL

logger = get_logger(__name__, LOG_LEVEL)



class ESConnector:
    def __init__(self, hosts: str, credentials: Tuple[str, str]):
        self.hosts = [hosts]
        self.credentials = credentials
        self.conn = self._create_es_connections()

    def _create_es_connections(self):
        username, password = self.credentials
        es = Elasticsearch(
            hosts=self.hosts,
            basic_auth=(username, password),
            verify_certs=False,
        )
        return es

    def ping(self):
        if self.es.ping():
            logger.info("Ping successful: Connected to Elasticsearch!")
        else:
            logger.info("Ping unsuccessful: Elasticsearch is not available!")


------------------

## processor/extractor.py

import os
import sys
from typing import Tuple, Optional, List

from google import genai
from google.genai import types

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from processor.prompts import EXTRACT_TEXT_PROMPT, EXTRACT_SUMMARY_PROMPT_1, EXTRACT_SUMMARY_PROMPT_2, EXTRACT_SUMMARY_PROMPT_3
from config import EXTRACT_TEXT_MODEL, EXTRACT_SUMMARY_MODEL

def load_image_as_bytes(image_path: str) -> bytes:
    with open(image_path, "rb") as f:
        return f.read()


def extract_text(image_path: str, client: genai.Client) -> Tuple[Optional[str], Optional[str]]:
    """
    Gemini를 이용해 이미지에서 Markdown 기반 텍스트 추출
    """
    try:
        prompt = EXTRACT_TEXT_PROMPT.strip()

        image_bytes = load_image_as_bytes(image_path)

        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text=prompt),
                    types.Part.from_bytes(data=image_bytes, mime_type="image/png"),
                ]
            )
        ]

        config = types.GenerateContentConfig(
            temperature=0,
            top_p=0.95,
            max_output_tokens=8192,
            response_modalities=["TEXT"],
            safety_settings=[
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="OFF"),
            ]
        )

        result = client.models.generate_content(
            model=EXTRACT_TEXT_MODEL,
            contents=contents,
            config=config
        )

        return result.text, None

    except Exception as e:
        return None, f"텍스트 추출 오류: {e}"


def extract_summary(target_image_path: str, context_image_paths: List[str], client: genai.Client) -> Tuple[Optional[str], Optional[str]]:
    """
    Gemini를 이용해 이미지 내용 요약/설명 추출
    """

    def get_description(file_name: str):
        description_mapping = {
            '1. International Standards': 'Includes international standard and specification documents required for high voltage circuit breaker design and testing. Refer to the technical specifications of global standardization organizations such as IEC and IEEE. Required data for product development and design standard establishment.',
            'IEC': 'Standard and specification documents on high-voltage equipment issued by the International Conference on Electrical Standards (IEC). Provide global standards for design, testing, safety standards, etc.',
            'IEEE': 'The American Institute of Electrical and Electronics (IEEE) specifications and standards, including market-focused design standards and testing procedures in North America.',
            '2. Type Test Reports': 'This is the type test result report of the actual manufactured circuit breaker. It is classified by model and year and details test items, insulation performance, and blocking performance.',
            '145SP-3': 'Type test data for circuit breaker model 145SP-3.',
            '145 kV 40 kA MS (2017)': 'A test report of a 145kV / 40kA class circuit breaker tested in 2017 of the model 145SP-3.',
            '300SR': 'Type test data for circuit breaker model 300SR.',
            '245 kV 50 kA MS (2020)': 'Test report of 245kV / 50kA circuit breaker tested in 2020 of Model 300SR.',
            '245 kV 63 kA MS (2024)': 'Test report of 245kV / 63kA circuit breaker carried out in 2024 of Model 300SR.',
            '3. Customer Standard Specifications': 'Standard Specifications by Country/Power Authority/Consumer.\nStandard Specification document provided by each country and customer. Refer to when delivering the product if you need a design that reflects customer requirements. This may include local application regulations and special requirements.',
            'Endeavour Energy': 'Standard specifications of Endeavour Energy, Power company in Australia.',
            'OETC': 'Standard specifications of OETC, Power company in Oman.',
            'SEC': 'Standard specifications of SEC, Power company in Saudi Arabia.',
            'Iberdrola': 'Standard specifications of Iberdrola, Power company in Spain.',
            'REE': 'Standard specifications of REE, Power company in Spain.',
        }
        keys = file_name.split('/')
        description = '\n'.join([description_mapping.get(k, "") for k in keys])
        return description

    try:
        parts = []

        prompt_1 = EXTRACT_SUMMARY_PROMPT_1.format(
            file_name=target_image_path,
            description=get_description(target_image_path)
        ).strip()
        parts.append(types.Part.from_text(text=prompt_1))

        for img_path in context_image_paths:
            image_bytes = load_image_as_bytes(img_path)
            parts.append(types.Part.from_bytes(data=image_bytes, mime_type="image/png"))

        prompt_2 = EXTRACT_SUMMARY_PROMPT_2.strip()
        parts.append(types.Part.from_text(text=prompt_2))

        image_bytes = load_image_as_bytes(target_image_path)
        parts.append(types.Part.from_bytes(data=image_bytes, mime_type="image/png"))
        
        prompt_3 = EXTRACT_SUMMARY_PROMPT_3.strip()
        parts.append(types.Part.from_text(text=prompt_3))

        contents = [
            types.Content(
                role="user",
                parts=parts,
            )
        ]        

        config = types.GenerateContentConfig(
            temperature=0.2,
            top_p=0.9,
            max_output_tokens=512,
            response_modalities=["TEXT"],
            safety_settings=[
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="OFF"),
            ]
        )

        result = client.models.generate_content(
            model=EXTRACT_SUMMARY_MODEL,
            contents=contents,
            config=config
        )

        return result.text, None

    except Exception as e:
        return None, f"요약 추출 오류: {e}"


------------------

## processor/pdf_manager.py

import os
import sys
import json
import tempfile
from typing import List, Optional, Tuple, Dict

from pdf2image import convert_from_path
from google import genai
from google.genai import types

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from storage.gcs_client import GCSStorageClient
from processor.extractor import extract_text, extract_summary
from processor.embedder import get_text_embedding
from processor.elastic import ESConnector
from db.repository import Repository
from db.models import PDFPage, PageStatus
from utils.utils import split_file_path
from utils.logger import get_logger
from config import LOG_LEVEL, INDEX_NAME



class PDFManager:
    def __init__(self, 
                 storage_client: GCSStorageClient, 
                 repository: Repository, 
                 genai_client: genai.Client,
                 els_client: ESConnector) -> None:
        self.storage = storage_client
        self.repo = repository
        self.genai = genai_client
        self.els =  els_client
        self.logger = get_logger(self.__class__.__name__, LOG_LEVEL)


    def invoke_split(self, gcs_pdf_path: str) -> Dict[str, str]:
        """
        GCS에 있는 PDF를 이미지로 분할한 후 GCS에 업로드
        {page_number: gcs_image_path} 딕셔너리 형태로 반환
        """
        with tempfile.TemporaryDirectory() as tmpdir:
            # PDF 임시 다운로드
            local_pdf_path = os.path.join(tmpdir, "doc.pdf")
            self.storage.download_file(gcs_pdf_path, local_pdf_path, self.storage.source_bucket)

            # PDF → 이미지 변환
            images = convert_from_path(local_pdf_path, dpi=300)

            # 이미지 GCS 업로드 경로 설정
            parent_dir, pdf_filename = split_file_path(gcs_pdf_path)
            pdf_basename = os.path.splitext(pdf_filename)[0]
            gcs_image_dir = f"{parent_dir}/{pdf_basename}" if parent_dir else pdf_basename
            
            page_infos: Dict[int, str] = {}  # {page_number: gcs_image_path}

            # 이미지 저장 및 GCS 업로드
            for i, image in enumerate(images, start=1):
                local_image_path = os.path.join(tmpdir, f"page-{i:05d}.png")
                image.save(local_image_path, "PNG")

                gcs_image_path = f"{gcs_image_dir}/{pdf_basename}-page-{i:05}.png"
                uploaded_path = self.storage.upload_file(local_image_path, gcs_image_path, self.storage.target_bucket)

                page_infos[i] = uploaded_path
        
            return page_infos


    def invoke_extraction(self, gcs_image_path: str) -> Tuple[str, str | None, PageStatus]:
        """
        Gemini를 이용해서 텍스트 추출 수행
        반환: (추출된 텍스트, 오류메시지, 상태)
        """
        try:
            with tempfile.TemporaryDirectory() as tmpdir:
                # 이미지 임시 다운로드
                local_image_path = os.path.join(tmpdir, os.path.basename(gcs_image_path))
                self.storage.download_file(gcs_image_path, local_image_path, self.storage.target_bucket)

                # 텍스트 추출
                text, error = extract_text(local_image_path, self.genai)
                status = PageStatus.SUCCESS if error is None else PageStatus.FAILED

                return text or "", error, status
        
        except Exception as e:
            return "", f"Extraction Exception: {e}", PageStatus.FAILED


    def invoke_summary(self, gcs_image_path: str) -> Tuple[str, str | None, PageStatus]:
        """
        Gemini를 이용해서 해당 이미지의 문서의 첫 5페이지를 참고해서 해당 페이지의 요약 수행
        반환: (요약된 텍스트, 오류메시지, 상태)
        """
        try:
            # doc_id 조회
            page = self.repo.session.query(PDFPage).filter_by(gcs_path=gcs_image_path).first()
            if not page:
                return "", f"DB에 해당 페이지 정보 없음: {gcs_image_path}", PageStatus.FAILED

            # 첫 5페이지 GCS Path 조회
            doc_id = page.doc_id
            gcs_context_paths = self.repo.get_first_n_pages(doc_id, 5)

            with tempfile.TemporaryDirectory() as tmpdir:
                # 현재 페이지 다운로드
                local_image_path = os.path.join(tmpdir, os.path.basename(gcs_image_path))
                self.storage.download_file(gcs_image_path, local_image_path, self.storage.target_bucket)

                # 컨텍스트 페이지 다운로드
                local_context_paths = []
                for gcs_path in gcs_context_paths:
                    local_path = os.path.join(tmpdir, os.path.basename(gcs_path))
                    self.storage.download_file(gcs_path, local_path, self.storage.target_bucket)
                    local_context_paths.append(local_path)

                # 요약 추출
                summary, error = extract_summary(local_image_path, local_context_paths, self.genai)
                status = PageStatus.SUCCESS if error is None else PageStatus.FAILED
                return summary or "", error, status
        
        except Exception as e:
            return "", f"Summary Exception: {e}", PageStatus.FAILED



    def invoke_embedding(self, gcs_image_path: str) -> Tuple[List[float] | None, str | None, PageStatus]:
        """
        텍스트 임베딩 벡터 생성
        반환: (embedding_vector, error_message, 상태)
        """
        # 페이지 정보 가져오기
        page = self.repo.session.query(PDFPage).filter_by(gcs_path=gcs_image_path).first()
        if not page:
            return None, f"DB에 해당 페이지 정보 없음: {gcs_image_path}", PageStatus.FAILED

        # 추출한 텍스트 확인
        combined_text = (page.summary or "") + "\n\n" + (page.extracted_text or "")
        if not combined_text.strip():
            return None, "임베딩 대상 텍스트가 비어있음", PageStatus.FAILED

        # 임베딩 수행
        try:
            embedding = get_text_embedding(combined_text, self.genai)
            return embedding, None, PageStatus.SUCCESS
        except Exception as e:
            return None, f"임베딩 오류: {e}", PageStatus.FAILED
    

    def invoke_indexing(self, page: PDFPage) -> Tuple[str, str, PageStatus, Optional[str]]:
        """
        ELS에 페이지 인덱싱 수행
        """
        try:
            data = {
                "page_id": page.page_id,
                "doc_id": page.doc_id,
                "page_number": page.page_number,
                "gcs_path": page.gcs_path,
                "gcs_pdf_path": page.gcs_pdf_path,
                "extracted_text": page.extracted_text,
                "summary": page.summary,
                "embedding": json.loads(page.embedding),
            }

            self.els.conn.index(index=INDEX_NAME, id=page.page_id, document=data)
            return page.page_id, page.gcs_path, PageStatus.SUCCESS, None

        except Exception as e:
            return page.page_id, page.gcs_path, PageStatus.FAILED, f"Indexing Error: {e}"

------------------

## processor/embedder.py

import os
import sys
from typing import Tuple, Optional, List

from google import genai
from google.genai.types import EmbedContentConfig

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from config import EMBEDDING_MODEL


def get_text_embedding(text: str, client: genai.Client) -> List[float]:
    response = client.models.embed_content(
        model=EMBEDDING_MODEL,
        contents=[text],
        config=EmbedContentConfig(
            task_type="RETRIEVAL_DOCUMENT",
            output_dimensionality=768,
            title="Content of Document"
        )
    )
    return response.embeddings[0].values


if __name__ == "__main__":
    ############### 함수 동작 테스트 ###############
    from config import PROJECT_ID, GENAI_LOCATION
    genai_client = genai.Client(
        vertexai=True,
        project=PROJECT_ID,
        location=GENAI_LOCATION,
    )

    res = get_text_embedding(text="안녕하세요", client=genai_client)
    print(f"length: {len(res)}")
    print(res[:5])

------------------

## scheduler/orchestrator.py

import os
import sys
import json
import time
from typing import List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

from google.cloud import storage
from google import genai

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from db.initialize import initialize_tables
from db.models import PageStatus
from db.session import get_db_session
from db.repository import Repository
from storage.gcs_client import GCSStorageClient
from processor.pdf_manager import PDFManager
from processor.elastic import ESConnector
from utils.logger import get_logger
from utils.utils import compute_doc_hash
from config import (
    GCS_SOURCE_BUCKET,
    GCS_PROCESSED_BUCKET,
    PROJECT_ID, 
    GENAI_LOCATION,
    ES_HOST,
    ES_USER,
    ES_PWD,
    INDEX_NAME,
    LOG_LEVEL
)

logger = get_logger(__name__, LOG_LEVEL)

def run_pipeline() -> None:

    logger.info("[Step 0] Initializing database tables")
    initialize_tables()


    db_gen = get_db_session()
    session = next(db_gen)

    try:
        # ── 초기화
        gcs_client = storage.Client()
        storage_client = GCSStorageClient(GCS_SOURCE_BUCKET, GCS_PROCESSED_BUCKET, gcs_client)
        repo = Repository(session)
        genai_client = genai.Client(vertexai=True, project=PROJECT_ID, location=GENAI_LOCATION)
        els = ESConnector(hosts=ES_HOST, credentials=(ES_USER, ES_PWD))

        manager = PDFManager(storage_client, repo, genai_client, els)


        # ─────────────────────────────────────────────────────────
        # 1. 신규문서 Detection
        # ─────────────────────────────────────────────────────────
        logger.info("[Step 1] Scanning GCS for new PDF documents")
        pdf_paths = storage_client.list_pdfs()
        logger.info(" └── Found %d PDF files in GCS", len(pdf_paths))
        
        known_doc_ids = repo.list_all_document_hashes()
        new_docs: List[Tuple[str, str]] = []

        for path in pdf_paths:
            try:
                doc_hash = compute_doc_hash(storage_client, path)
                if doc_hash not in known_doc_ids:
                    new_docs.append((doc_hash, path))
            except Exception as e:
                logger.warning(" └── Hash computation failed for %s (%s)", path, e)

        logger.info(" └── Detected %d new documents", len(new_docs))


        # ─────────────────────────────────────────────────────────
        # 2. 신규문서 Split해서 DB 등록
        # ─────────────────────────────────────────────────────────
        logger.info("[Step 2] Splitting new documents and saving page metadata")

        # 병렬처리 미적용
        for i, (doc_id, gcs_pdf_path) in enumerate(new_docs):
            try:
                # PDFDocument Table 등록
                if not repo.exists_document(doc_id):
                    repo.create_document(doc_id, gcs_pdf_path)

                # PDFPage Table 등록
                gcs_page_infos = manager.invoke_split(gcs_pdf_path) # {page_number: gcs_image_path}                
                for page_number, gcs_image_path in gcs_page_infos.items():
                    repo.create_page_record(
                        doc_id=doc_id,
                        page_number=page_number,
                        gcs_path=gcs_image_path,
                        gcs_pdf_path=gcs_pdf_path,
                    )  

                logger.info(" └── [%d/%d] Split and saved %d pages: %s", i + 1, len(new_docs), len(gcs_page_infos), gcs_pdf_path)

            except Exception as e:
                logger.warning(" └── [%d/%d] Failed to split or save: %s (%s)", i + 1, len(new_docs), gcs_pdf_path, e)


        # ─────────────────────────────────────────────────────────
        # 3. 텍스트 추출
        # ─────────────────────────────────────────────────────────
        logger.info("[Step 3] Extracting text from page images")
        extraction_pages = repo.get_pages_for_extraction()
        pending = [p for p in extraction_pages if p.extracted == PageStatus.PENDING]
        retry   = [p for p in extraction_pages if p.extracted == PageStatus.FAILED]
        logger.info("Pages queued for text extraction: %d (new: %d, retry: %d)", len(extraction_pages), len(pending), len(retry))

        # 병렬처리 미적용
        for i, page in enumerate(extraction_pages, 1):
            try:
                tag = "new" if page.extracted == PageStatus.PENDING else "retry"
                text, error, status = manager.invoke_extraction(page.gcs_path)

                repo.update_page_record(
                    page_id=page.page_id,
                    extracted_text=text,
                    extracted=status,
                    error_message=error
                )   

                if status == PageStatus.SUCCESS:
                    logger.debug(" └── [%d/%d] Text extraction succeeded (%s): %s", i, len(extraction_pages), tag, page.gcs_path)
                else:
                    logger.warning(" └── [%d/%d] Text extraction failed (%s): %s - %s", i, len(extraction_pages), tag, page.gcs_path, error)
                    time.sleep(60)

            except Exception as e:
                logger.error(" └── [%d/%d] Text extraction exception (%s): %s - %s", i, len(extraction_pages), tag, page.gcs_path, e)


        # ─────────────────────────────────────────────────────────
        # 4. 요약 추출
        # ─────────────────────────────────────────────────────────
        logger.info("[Step 4] Generating summaries")
        summary_pages = repo.get_pages_for_summary()
        pending = [p for p in summary_pages if p.summarized == PageStatus.PENDING]
        retry   = [p for p in summary_pages if p.summarized == PageStatus.FAILED]
        logger.info("Pages queued for summary generation: %d (new: %d, retry: %d)", len(summary_pages), len(pending), len(retry))

        # 병렬처리 미적용
        for i, page in enumerate(summary_pages, 1):
            try:
                tag = "new" if page.summarized == PageStatus.PENDING else "retry"
                summary, error, status = manager.invoke_summary(page.gcs_path)

                repo.update_page_record(
                    page_id=page.page_id,
                    summary=summary,
                    summarized=status,
                    error_message=error
                )

                if status == PageStatus.SUCCESS:
                    logger.debug(" └── [%d/%d] Summary generation succeeded (%s): %s", i, len(summary_pages), tag, page.gcs_path)
                else:
                    logger.warning(" └── [%d/%d] Summary generation failed (%s): %s - %s", i, len(summary_pages), tag, page.gcs_path, error)
                    time.sleep(60)

            except Exception as e:
                logger.error(" └── [%d/%d] Summary generation exception (%s): %s - %s", i, len(summary_pages), tag, page.gcs_path, e)



        # ─────────────────────────────────────────────────────────
        # 5. 임베딩 벡터 생성
        # ─────────────────────────────────────────────────────────
        logger.info("[Step 5] Generating embeddings")
        embedding_pages = repo.get_pages_for_embedding()
        pending = [p for p in embedding_pages if p.embedded == PageStatus.PENDING]
        retry   = [p for p in embedding_pages if p.embedded == PageStatus.FAILED]
        logger.info("Pages queued for embedding: %d (new: %d, retry: %d)", len(embedding_pages), len(pending), len(retry))

        # 병렬처리 미적용
        for i, page in enumerate(embedding_pages, 1):
            try:
                tag = "new" if page.embedded == PageStatus.PENDING else "retry"
                embedding, error, status = manager.invoke_embedding(page.gcs_path)

                repo.update_page_record(
                    page_id=page.page_id,
                    embedding=json.dumps(embedding), # 리스트를 문자열로 변환
                    embedded=status,
                    error_message=error
                )

                if status == PageStatus.SUCCESS:
                    logger.debug(" └── [%d/%d] Embedding succeeded (%s): %s", i, len(embedding_pages), tag, page.gcs_path)
                else:
                    logger.warning(" └── [%d/%d] Embedding failed (%s): %s - %s", i, len(embedding_pages), tag, page.gcs_path, error)
                    time.sleep(60)

            except Exception as e:
                logger.error(" └── [%d/%d] Embedding exception (%s): %s - %s", i, len(embedding_pages), tag, page.gcs_path, e)



        # ─────────────────────────────────────────────────────────
        # 6. 인덱싱
        # ─────────────────────────────────────────────────────────
        logger.info("[Step 6] Indexing")
        indexing_pages = repo.get_pages_for_indexing()
        pending = [p for p in embedding_pages if p.indexed == PageStatus.PENDING]
        retry   = [p for p in embedding_pages if p.indexed == PageStatus.FAILED]
        logger.info("Pages queued for indexing: %d (new: %d, retry: %d)", len(embedding_pages), len(pending), len(retry))

        # 병렬처리 미적용
        for i, page in enumerate(indexing_pages, 1):
            try:
                tag = "new" if page.indexed == PageStatus.PENDING else "retry"
                page_id, gcs_page_path, status, error = manager.invoke_indexing(page)

                repo.update_page_record(
                    page_id=page_id,
                    indexed=status,
                    error_message=error
                )

                if status == PageStatus.SUCCESS:
                    logger.debug(" └── [%d/%d] Indexing succeeded (%s): %s", i, len(indexing_pages), tag, page.gcs_path)
                else:
                    logger.warning(" └── [%d/%d] Indexing failed (%s): %s - %s", i, len(indexing_pages), tag, page.gcs_path, error)

            except Exception as e:
                logger.error(" └── [%d/%d] Indexing exception (%s): %s - %s", i, len(indexing_pages), tag, page.gcs_path, e)


    finally:
        session.close()
        logger.info("\nPipeline execution finished")

------------------

## storage/gcs_client.py

from typing import List
from google.cloud import storage

class GCSStorageClient:
    def __init__(self, source_bucket: str, target_bucket: str, client: storage.Client) -> None:
        self.source_bucket: str = source_bucket
        self.target_bucket: str = target_bucket
        self.client: storage.Client = client
    
    def list_pdfs(self, prefix: str = "") -> List[str]:
        bucket = self.client.bucket(self.source_bucket)
        blobs = bucket.list_blobs(prefix=prefix)
        return [blob.name for blob in blobs if blob.name.lower().endswith(".pdf")]
    
    def download_file(self, gcs_path: str, local_path: str, bucket_name: str) -> str:
        bucket = self.client.bucket(bucket_name)
        blob = bucket.blob(gcs_path)
        blob.download_to_filename(local_path)
        return local_path
    
    def upload_file(self, local_image_path: str, gcs_path: str, bucket_name: str) -> str:
        bucket = self.client.bucket(bucket_name)
        blob = bucket.blob(gcs_path)
        blob.upload_from_filename(local_image_path)
        return gcs_path
    
    def make_output_path(self, src_gcs_path: str, page_num: int) -> str:
        parts = src_gcs_path.rsplit("/", 1)
        dirs = parts[0] if len(parts) > 1 else ""
        filename = parts[-1]
        base = filename.rsplit(".", 1)[0]
        folder = f"{dirs}/{base}" if dirs else base
        page_fname = f"{base}-page-{page_num:05d}.png"
        return f"{folder}/{page_fname}"
    

------------------

## utils/utils.py

import os
import sys
import hashlib

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from storage.gcs_client import GCSStorageClient


def get_file_hash(file_path, hash_type='sha256'):
    # 사용할 해시 알고리즘 선택
    hash_func = getattr(hashlib, hash_type)()

    # 파일을 바이너리 모드로 읽으면서 해시 업데이트
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_func.update(chunk)

    return hash_func.hexdigest()


def compute_doc_hash(storage_client: GCSStorageClient, gcs_pdf_path: str) -> str:
    """Download a PDF to a tmp file and return its sha-256 hash."""
    import tempfile
    with tempfile.NamedTemporaryFile(suffix=".pdf") as tmp:
        storage_client.download_file(gcs_pdf_path, tmp.name, storage_client.source_bucket)
        return get_file_hash(tmp.name)

def split_file_path(path: str):
    """
    경로 분리: "folder/subfolder/doc.pdf" → "folder/subfolder", "doc.pdf"
    """
    if "/" in path:
        parent_dir, filename = path.rsplit("/", 1)
    else:
        parent_dir, filename = "", path
    return parent_dir, filename




# --- Git Utils
import os
from pathspec import PathSpec
from pathspec.patterns import GitWildMatchPattern

def load_gitignore(path=".gitignore"):
    if not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8") as f:
        patterns = f.read().splitlines()
    return PathSpec.from_lines(GitWildMatchPattern, patterns)

def generate_tree(dir_path, prefix="", spec=None, base_path=""):
    tree_str = ""
    entries = sorted(os.listdir(dir_path))
    entries = [e for e in entries if not e.startswith('.')]  # 숨김 파일 기본 제거

    for index, entry in enumerate(entries):
        full_path = os.path.join(dir_path, entry)
        relative_path = os.path.relpath(full_path, base_path)

        if spec and spec.match_file(relative_path):
            continue  # .gitignore에 해당되면 스킵

        connector = "└── " if index == len(entries) - 1 else "├── "
        tree_str += f"{prefix}{connector}{entry}\n"

        if os.path.isdir(full_path):
            extension = "    " if index == len(entries) - 1 else "│   "
            tree_str += generate_tree(full_path, prefix + extension, spec, base_path)
    return tree_str

def save_to_readme(tree_str, readme_path="README.md"):
    with open(readme_path, "a", encoding="utf-8") as f:
        f.write("\n```\n")
        f.write(tree_str)
        f.write("```\n")

if __name__ == "__main__":
    base_dir = "."
    gitignore_spec = load_gitignore()
    tree_output = generate_tree(base_dir, spec=gitignore_spec, base_path=base_dir)
    save_to_readme(tree_output)
    print("📁 .gitignore 제외하고 README.md에 구조를 추가했습니다.")

------------------

## utils/logger.py

import logging
import sys
from typing import Optional

# _FMT = "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s"
_FMT = "%(asctime)s | %(levelname)-8s | %(message)s"


def get_logger(name: str,
               level: str | int = "INFO",
               stream: Optional[object] = None) -> logging.Logger:
    logger = logging.getLogger(name)
    # if already configured – return as‑is
    if logger.handlers:
        return logger

    handler = logging.StreamHandler(stream or sys.stdout)
    handler.setFormatter(logging.Formatter(_FMT))
    logger.addHandler(handler)
    logger.setLevel(level)
    logger.propagate = False  # stop double‑logs
    return logger

------------------

## db/initialize.py

import os
import sys
from sqlalchemy import inspect

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from db.models import Base
from db.session import engine
from utils.logger import get_logger
from config import LOG_LEVEL

logger = get_logger(__name__, LOG_LEVEL)


def print_table_infos():
    inspector = inspect(engine)
    tables = inspector.get_table_names()

    print("\n현재 DB 테이블 및 컬럼 정보:")
    for table in tables:
        print(f"\nTABLE NAME: {table}")
        columns = inspector.get_columns(table)
        for col in columns:
            name = col['name']
            type_ = col['type']
            nullable = col['nullable']
            default = col.get('default', None)
            print(f"  - {name} ({type_}){' NULL' if nullable else ' NOT NULL'}"
                  f"{' DEFAULT ' + str(default) if default is not None else ''}")
    return tables

def initialize_tables():
    inspector = inspect(engine)
    existing_tables = inspector.get_table_names()

    # 최소한의 기준 테이블 존재 여부 판단
    if "pdf_documents" not in existing_tables or "pdf_pages" not in existing_tables:
        logger.info(" ┌── There is no tables → Try to create tables")
        Base.metadata.create_all(bind=engine)
        logger.info(" └── Created tables!")
    else:
        logger.info(" └── Already exists all tables")
    
    # print_table_infos()


if __name__ == "__main__":
   initialize_tables()

------------------

## db/repository.py

import os
import sys
from typing import List

from sqlalchemy import distinct
from sqlalchemy.orm import Session
from sqlalchemy import select

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from db.models import PDFDocument, PDFPage, PageStatus
from storage.gcs_client import GCSStorageClient
from utils.logger import get_logger
from config import LOG_LEVEL

class Repository:
    def __init__(self, session: Session) -> None:
        self.session = session
        self.logger = get_logger(self.__class__.__name__, LOG_LEVEL)


    def list_all_document_hashes(self) -> set[str]:
        result = (
            self.session.query(PDFDocument.doc_id)
            .all()
        )
        return {row[0] for row in result}


    def exists_document(self, doc_id: str) -> bool:
        return self.session.query(PDFDocument).filter_by(doc_id=doc_id).first() is not None


    def create_document(self, doc_id: str, gcs_path: str) -> PDFDocument:
        doc = PDFDocument(doc_id=doc_id, gcs_path=gcs_path)
        self.session.add(doc)
        self.session.commit()
        return doc


    def create_page_record(self, doc_id: str, page_number:int, gcs_path: str, gcs_pdf_path: str) -> PDFPage:
        page = PDFPage(
            page_id=f"{doc_id}_{page_number:05d}",
            doc_id=doc_id,
            page_number=f"{page_number:05d}",
            gcs_path=gcs_path,
            gcs_pdf_path=gcs_pdf_path
        )
        self.session.add(page)
        self.session.commit()
        return page


    def update_page_record(self, page_id: str, **kwargs):
        page = self.session.get(PDFPage, page_id)
        if page:
            for k, v in kwargs.items():
                if v is not None:
                    setattr(page, k, v)
            self.session.commit()


    def get_first_n_pages(self, doc_id: str, n: int = 5) -> List[str]:
        stmt = (
            select(PDFPage.gcs_path)
            .where(PDFPage.doc_id == doc_id)
            .order_by(PDFPage.page_number.asc())
            .limit(n)
        )
        return self.session.scalars(stmt).all()


    def get_pages_for_extraction(self) -> List[PDFPage]:
        return (
            self.session.query(PDFPage)
            .filter(PDFPage.extracted.in_([PageStatus.PENDING, PageStatus.FAILED]))
            .all()
        )


    def get_pages_for_summary(self) -> List[PDFPage]:
        return (
            self.session.query(PDFPage)
            .filter(PDFPage.summarized.in_([PageStatus.PENDING, PageStatus.FAILED]))
            .all()
        )

    def get_pages_for_embedding(self) -> List[PDFPage]:
        return (
            self.session.query(PDFPage)
            .filter(PDFPage.extracted == PageStatus.SUCCESS)
            .filter(PDFPage.summarized == PageStatus.SUCCESS)
            .filter(PDFPage.embedded.in_([PageStatus.PENDING, PageStatus.FAILED]))
            .all()
        )


    def get_pages_for_indexing(self) -> List[PDFPage]:
        return (
            self.session.query(PDFPage)
            .filter(PDFPage.embedded == PageStatus.SUCCESS)
            .filter(PDFPage.indexed.in_([PageStatus.PENDING, PageStatus.FAILED]))
            .all()
        )

------------------

## db/models.py

import os
import sys
import enum
from datetime import datetime

from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy import (
    Column, Integer, String, Text, DateTime, Enum, ForeignKey
)
from sqlalchemy.dialects.mysql import LONGTEXT
from sqlalchemy.sql import func

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from config import TABLENAME_PDFPAGE, TABLENAME_PDFDOCUMENT


# 모든 ORM 모델의 기본이 되는 클래스를 정의
# 아래의 테이블 클래스들이 이 Base를 상속해서 만들어야 실제 테이블로 인식됨
Base = declarative_base()

class PageStatus(enum.Enum):
    PENDING = "PENDING"     # 아직 시도 안 함
    SUCCESS = "SUCCESS"     # 처리 완료
    FAILED = "FAILED"       # 처리 실패

class PDFPage(Base):
    __tablename__ = TABLENAME_PDFPAGE

    page_id: int = Column(String(128), primary_key=True)
    doc_id: str = Column(String(128), ForeignKey("pdf_documents.doc_id"), nullable=False)
    page_number: str = Column(String(32), nullable=False)
    
    gcs_path: str = Column(String(1000), nullable=False)
    gcs_pdf_path: str = Column(String(1000), nullable=False)
    
    extracted_text: str = Column(LONGTEXT, nullable=True)
    summary: str = Column(LONGTEXT, nullable=True)
    embedding: str = Column(LONGTEXT, nullable=True)
    
    extracted: PageStatus = Column(Enum(PageStatus), default=PageStatus.PENDING)
    summarized: PageStatus = Column(Enum(PageStatus), default=PageStatus.PENDING)
    embedded: PageStatus = Column(Enum(PageStatus), default=PageStatus.PENDING)
    indexed: PageStatus = Column(Enum(PageStatus), default=PageStatus.PENDING)

    error_message: str = Column(Text, nullable=True)
    created_at: datetime = Column(DateTime, default=datetime.utcnow)
    updated_at: datetime = Column(DateTime, default=func.now(), onupdate=func.now())

    document = relationship("PDFDocument", back_populates="pages")


class PDFDocument(Base):
    __tablename__ = TABLENAME_PDFDOCUMENT

    doc_id: str = Column(String(128), primary_key=True)
    gcs_path: str = Column(String(1000), nullable=False)
    created_at: datetime = Column(DateTime, default=datetime.utcnow)

    pages = relationship("PDFPage", back_populates="document", cascade="all, delete-orphan")
        # back_populates="document"     : document.pages로 페이지 접근 가능, page.document로 해당 페이지가 속한 문서 확인가능
        # cascade="all, delete-orphan"  : 부모(PDFDocument)가 삭제되었을때 자식(PDFPage)도 자동으로 처리되도록

------------------

## db/session.py

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session
from typing import Generator
from config import (
    MYSQL_USER, MYSQL_PWD, MYSQL_HOST,
    MYSQL_PORT, MYSQL_DB, MYSQL_CHARSET
)

# SQLAlchemy + PyMySQL 연결 문자열 생성
# DATABASE_URL: str = (
#     f"mysql+pymysql://{MYSQL_USER}:{MYSQL_PWD}"
#     f"@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}"
#     f"?charset={MYSQL_CHARSET}"
# )
DATABASE_URL = "mysql+pymysql://hde_chat:dpdlcldecot1%40@10.100.79.63:23306/hde_chat?charset=utf8mb4"


# 엔진 생성 (pool_pre_ping=True 로 연결 끊김 방지)
engine = create_engine(
    DATABASE_URL,
    echo=False,
    pool_pre_ping=True
)

# 세션 팩토리
SessionLocal: sessionmaker = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine
)

def get_db_session() -> Generator[Session, None, None]:
    """
    Dependency 또는 직접 호출용으로,
    yield 후 반드시 .close() 되도록 설계.
    """
    session: Session = SessionLocal()
    try:
        yield session
    finally:
        session.close()

------------------
`