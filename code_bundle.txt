`# Project Structure

â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ code_bundle.txt
â”œâ”€â”€ config.py
â”œâ”€â”€ db
â”‚   â”œâ”€â”€ initialize.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ repository.py
â”‚   â””â”€â”€ session.py
â”œâ”€â”€ key
â”‚   â””â”€â”€ pjt-dev-hdegis-app-454401-bd4fac2d452b.json
â”œâ”€â”€ main.py
â”œâ”€â”€ nohup-main.out
â”œâ”€â”€ processor
â”‚   â”œâ”€â”€ elastic.py
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ extractor.py
â”‚   â”œâ”€â”€ pdf_manager.py
â”‚   â””â”€â”€ prompts.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ sample.png
â”œâ”€â”€ scheduler
â”‚   â”œâ”€â”€ orchestator_parallel.py
â”‚   â””â”€â”€ orchestrator.py
â”œâ”€â”€ storage
â”‚   â””â”€â”€ gcs_client.py
â”œâ”€â”€ test_ieee.pdf
â””â”€â”€ utils
    â”œâ”€â”€ logger.py
    â””â”€â”€ utils.py

------------------

## main.py

from scheduler.orchestrator import run_pipeline

def main() -> None:
    run_pipeline()

if __name__ == "__main__":
    main()

------------------

## config.py

from dotenv import load_dotenv
import os

load_dotenv()

# GCP
GOOGLE_APPLICATION_CREDENTIALS: str = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
PROJECT_ID: str = os.getenv("PROJECT_ID")

GCS_SOURCE_BUCKET: str = os.getenv("GCS_SOURCE_BUCKET")
GCS_PROCESSED_BUCKET: str = os.getenv("GCS_PROCESSED_BUCKET")

GENAI_LOCATION: str = os.getenv("GENAI_LOCATION")
EXTRACT_TEXT_MODEL: str = os.getenv("EXTRACT_TEXT_MODEL")
EXTRACT_SUMMARY_MODEL: str = os.getenv("EXTRACT_SUMMARY_MODEL")
EMBEDDING_MODEL: str = os.getenv("EMBEDDING_MODEL")



# MySQL
MYSQL_HOST: str = os.getenv("MYSQL_HOST")
MYSQL_PORT: int = int(os.getenv("MYSQL_PORT"))
MYSQL_USER: str = os.getenv("MYSQL_USER")
MYSQL_PWD: str = os.getenv("MYSQL_PWD")
MYSQL_CHARSET: str = os.getenv("MYSQL_CHARSET")
MYSQL_DB: str = os.getenv("MYSQL_DB")

TABLENAME_PDFPAGE: str = os.getenv("TABLENAME_PDFPAGE")
TABLENAME_PDFDOCUMENT: str = os.getenv("TABLENAME_PDFDOCUMENT")



# Elastic
ES_HOST: str = os.getenv("ES_HOST")
ES_USER: str = os.getenv("ES_USER")
ES_PWD: str = os.getenv("ES_PWD")
CA_CERT: str = os.getenv("CA_CERT")

INDEX_NAME: str = f"hdegis-{EMBEDDING_MODEL}"

# LOG
LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO")

------------------

## processor/elastic.py

import os
import sys
from typing import List, Dict, Tuple, Optional, Any
import warnings
from urllib3.exceptions import InsecureRequestWarning

# InsecureRequestWarning ê²½ê³  ë¬´ì‹œ
warnings.simplefilter("ignore", InsecureRequestWarning)

from elasticsearch import Elasticsearch

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from utils.logger import get_logger
from config import LOG_LEVEL

logger = get_logger(__name__, LOG_LEVEL)



class ESConnector:
    def __init__(self, hosts: str, credentials: Tuple[str, str]):
        self.hosts = [hosts]
        self.credentials = credentials
        self.conn = self._create_es_connections()

    def _create_es_connections(self):
        username, password = self.credentials
        es = Elasticsearch(
            hosts=self.hosts,
            basic_auth=(username, password),
            verify_certs=False,
        )
        return es

    def ping(self):
        if self.es.ping():
            logger.info("Ping successful: Connected to Elasticsearch!")
        else:
            logger.info("Ping unsuccessful: Elasticsearch is not available!")


------------------

## processor/extractor.py

import os
import sys
from typing import Tuple, Optional, List

from google import genai
from google.genai import types

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from processor.prompts import EXTRACT_TEXT_PROMPT, EXTRACT_SUMMARY_PROMPT_1, EXTRACT_SUMMARY_PROMPT_2, EXTRACT_SUMMARY_PROMPT_3
from config import EXTRACT_TEXT_MODEL, EXTRACT_SUMMARY_MODEL

def load_image_as_bytes(image_path: str) -> bytes:
    with open(image_path, "rb") as f:
        return f.read()


def extract_text(image_path: str, client: genai.Client) -> Tuple[Optional[str], Optional[str]]:
    """
    Geminië¥¼ ì´ìš©í•´ ì´ë¯¸ì§€ì—ì„œ Markdown ê¸°ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    try:
        prompt = EXTRACT_TEXT_PROMPT.strip()

        image_bytes = load_image_as_bytes(image_path)

        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text=prompt),
                    types.Part.from_bytes(data=image_bytes, mime_type="image/png"),
                ]
            )
        ]

        config = types.GenerateContentConfig(
            temperature=0,
            top_p=0.95,
            max_output_tokens=8192,
            response_modalities=["TEXT"],
            safety_settings=[
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="OFF"),
            ]
        )

        result = client.models.generate_content(
            model=EXTRACT_TEXT_MODEL,
            contents=contents,
            config=config
        )

        return result.text, None

    except Exception as e:
        return None, f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}"


def extract_summary(target_image_path: str, context_image_paths: List[str], client: genai.Client) -> Tuple[Optional[str], Optional[str]]:
    """
    Geminië¥¼ ì´ìš©í•´ ì´ë¯¸ì§€ ë‚´ìš© ìš”ì•½/ì„¤ëª… ì¶”ì¶œ
    """

    def get_description(file_name: str):
        description_mapping = {
            '1. International Standards': 'Includes international standard and specification documents required for high voltage circuit breaker design and testing. Refer to the technical specifications of global standardization organizations such as IEC and IEEE. Required data for product development and design standard establishment.',
            'IEC': 'Standard and specification documents on high-voltage equipment issued by the International Conference on Electrical Standards (IEC). Provide global standards for design, testing, safety standards, etc.',
            'IEEE': 'The American Institute of Electrical and Electronics (IEEE) specifications and standards, including market-focused design standards and testing procedures in North America.',
            '2. Type Test Reports': 'This is the type test result report of the actual manufactured circuit breaker. It is classified by model and year and details test items, insulation performance, and blocking performance.',
            '145SP-3': 'Type test data for circuit breaker model 145SP-3.',
            '145 kV 40 kA MS (2017)': 'A test report of a 145kV / 40kA class circuit breaker tested in 2017 of the model 145SP-3.',
            '300SR': 'Type test data for circuit breaker model 300SR.',
            '245 kV 50 kA MS (2020)': 'Test report of 245kV / 50kA circuit breaker tested in 2020 of Model 300SR.',
            '245 kV 63 kA MS (2024)': 'Test report of 245kV / 63kA circuit breaker carried out in 2024 of Model 300SR.',
            '3. Customer Standard Specifications': 'Standard Specifications by Country/Power Authority/Consumer.\nStandard Specification document provided by each country and customer. Refer to when delivering the product if you need a design that reflects customer requirements. This may include local application regulations and special requirements.',
            'Endeavour Energy': 'Standard specifications of Endeavour Energy, Power company in Australia.',
            'OETC': 'Standard specifications of OETC, Power company in Oman.',
            'SEC': 'Standard specifications of SEC, Power company in Saudi Arabia.',
            'Iberdrola': 'Standard specifications of Iberdrola, Power company in Spain.',
            'REE': 'Standard specifications of REE, Power company in Spain.',
        }
        keys = file_name.split('/')
        description = '\n'.join([description_mapping.get(k, "") for k in keys])
        return description

    try:
        parts = []

        prompt_1 = EXTRACT_SUMMARY_PROMPT_1.format(
            file_name=target_image_path,
            description=get_description(target_image_path)
        ).strip()
        parts.append(types.Part.from_text(text=prompt_1))

        for img_path in context_image_paths:
            image_bytes = load_image_as_bytes(img_path)
            parts.append(types.Part.from_bytes(data=image_bytes, mime_type="image/png"))

        prompt_2 = EXTRACT_SUMMARY_PROMPT_2.strip()
        parts.append(types.Part.from_text(text=prompt_2))

        image_bytes = load_image_as_bytes(target_image_path)
        parts.append(types.Part.from_bytes(data=image_bytes, mime_type="image/png"))
        
        prompt_3 = EXTRACT_SUMMARY_PROMPT_3.strip()
        parts.append(types.Part.from_text(text=prompt_3))

        contents = [
            types.Content(
                role="user",
                parts=parts,
            )
        ]        

        config = types.GenerateContentConfig(
            temperature=0.2,
            top_p=0.9,
            max_output_tokens=512,
            response_modalities=["TEXT"],
            safety_settings=[
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="OFF"),
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="OFF"),
            ]
        )

        result = client.models.generate_content(
            model=EXTRACT_SUMMARY_MODEL,
            contents=contents,
            config=config
        )

        return result.text, None

    except Exception as e:
        return None, f"ìš”ì•½ ì¶”ì¶œ ì˜¤ë¥˜: {e}"


------------------

## processor/pdf_manager.py

import os
import sys
import json
import tempfile
from typing import List, Optional, Tuple, Dict

from pdf2image import convert_from_path
from google import genai
from google.genai import types

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from storage.gcs_client import GCSStorageClient
from processor.extractor import extract_text, extract_summary
from processor.embedder import get_text_embedding
from processor.elastic import ESConnector
from db.repository import Repository
from db.models import PDFPage, PageStatus
from utils.utils import split_file_path
from utils.logger import get_logger
from config import LOG_LEVEL, INDEX_NAME



class PDFManager:
    def __init__(self, 
                 storage_client: GCSStorageClient, 
                 repository: Repository, 
                 genai_client: genai.Client,
                 els_client: ESConnector) -> None:
        self.storage = storage_client
        self.repo = repository
        self.genai = genai_client
        self.els =  els_client
        self.logger = get_logger(self.__class__.__name__, LOG_LEVEL)


    def invoke_split(self, gcs_pdf_path: str) -> Dict[str, str]:
        """
        GCSì— ìˆëŠ” PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë¶„í• í•œ í›„ GCSì— ì—…ë¡œë“œ
        {page_number: gcs_image_path} ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜
        """
        with tempfile.TemporaryDirectory() as tmpdir:
            # PDF ì„ì‹œ ë‹¤ìš´ë¡œë“œ
            local_pdf_path = os.path.join(tmpdir, "doc.pdf")
            self.storage.download_file(gcs_pdf_path, local_pdf_path, self.storage.source_bucket)

            # PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
            images = convert_from_path(local_pdf_path, dpi=300)

            # ì´ë¯¸ì§€ GCS ì—…ë¡œë“œ ê²½ë¡œ ì„¤ì •
            parent_dir, pdf_filename = split_file_path(gcs_pdf_path)
            pdf_basename = os.path.splitext(pdf_filename)[0]
            gcs_image_dir = f"{parent_dir}/{pdf_basename}" if parent_dir else pdf_basename
            
            page_infos: Dict[int, str] = {}  # {page_number: gcs_image_path}

            # ì´ë¯¸ì§€ ì €ì¥ ë° GCS ì—…ë¡œë“œ
            for i, image in enumerate(images, start=1):
                local_image_path = os.path.join(tmpdir, f"page-{i:05d}.png")
                image.save(local_image_path, "PNG")

                gcs_image_path = f"{gcs_image_dir}/{pdf_basename}-page-{i:05}.png"
                uploaded_path = self.storage.upload_file(local_image_path, gcs_image_path, self.storage.target_bucket)

                page_infos[i] = uploaded_path
        
            return page_infos


    def invoke_extraction(self, gcs_image_path: str) -> Tuple[str, str | None, PageStatus]:
        """
        Geminië¥¼ ì´ìš©í•´ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ìˆ˜í–‰
        ë°˜í™˜: (ì¶”ì¶œëœ í…ìŠ¤íŠ¸, ì˜¤ë¥˜ë©”ì‹œì§€, ìƒíƒœ)
        """
        try:
            with tempfile.TemporaryDirectory() as tmpdir:
                # ì´ë¯¸ì§€ ì„ì‹œ ë‹¤ìš´ë¡œë“œ
                local_image_path = os.path.join(tmpdir, os.path.basename(gcs_image_path))
                self.storage.download_file(gcs_image_path, local_image_path, self.storage.target_bucket)

                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text, error = extract_text(local_image_path, self.genai)
                status = PageStatus.SUCCESS if error is None else PageStatus.FAILED

                return text or "", error, status
        
        except Exception as e:
            return "", f"Extraction Exception: {e}", PageStatus.FAILED


    def invoke_summary(self, gcs_image_path: str) -> Tuple[str, str | None, PageStatus]:
        """
        Geminië¥¼ ì´ìš©í•´ì„œ í•´ë‹¹ ì´ë¯¸ì§€ì˜ ë¬¸ì„œì˜ ì²« 5í˜ì´ì§€ë¥¼ ì°¸ê³ í•´ì„œ í•´ë‹¹ í˜ì´ì§€ì˜ ìš”ì•½ ìˆ˜í–‰
        ë°˜í™˜: (ìš”ì•½ëœ í…ìŠ¤íŠ¸, ì˜¤ë¥˜ë©”ì‹œì§€, ìƒíƒœ)
        """
        try:
            # doc_id ì¡°íšŒ
            page = self.repo.session.query(PDFPage).filter_by(gcs_path=gcs_image_path).first()
            if not page:
                return "", f"DBì— í•´ë‹¹ í˜ì´ì§€ ì •ë³´ ì—†ìŒ: {gcs_image_path}", PageStatus.FAILED

            # ì²« 5í˜ì´ì§€ GCS Path ì¡°íšŒ
            doc_id = page.doc_id
            gcs_context_paths = self.repo.get_first_n_pages(doc_id, 5)

            with tempfile.TemporaryDirectory() as tmpdir:
                # í˜„ì¬ í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ
                local_image_path = os.path.join(tmpdir, os.path.basename(gcs_image_path))
                self.storage.download_file(gcs_image_path, local_image_path, self.storage.target_bucket)

                # ì»¨í…ìŠ¤íŠ¸ í˜ì´ì§€ ë‹¤ìš´ë¡œë“œ
                local_context_paths = []
                for gcs_path in gcs_context_paths:
                    local_path = os.path.join(tmpdir, os.path.basename(gcs_path))
                    self.storage.download_file(gcs_path, local_path, self.storage.target_bucket)
                    local_context_paths.append(local_path)

                # ìš”ì•½ ì¶”ì¶œ
                summary, error = extract_summary(local_image_path, local_context_paths, self.genai)
                status = PageStatus.SUCCESS if error is None else PageStatus.FAILED
                return summary or "", error, status
        
        except Exception as e:
            return "", f"Summary Exception: {e}", PageStatus.FAILED



    def invoke_embedding(self, gcs_image_path: str) -> Tuple[List[float] | None, str | None, PageStatus]:
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¡í„° ìƒì„±
        ë°˜í™˜: (embedding_vector, error_message, ìƒíƒœ)
        """
        # í˜ì´ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        page = self.repo.session.query(PDFPage).filter_by(gcs_path=gcs_image_path).first()
        if not page:
            return None, f"DBì— í•´ë‹¹ í˜ì´ì§€ ì •ë³´ ì—†ìŒ: {gcs_image_path}", PageStatus.FAILED

        # ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ í™•ì¸
        combined_text = (page.summary or "") + "\n\n" + (page.extracted_text or "")
        if not combined_text.strip():
            return None, "ì„ë² ë”© ëŒ€ìƒ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ", PageStatus.FAILED

        # ì„ë² ë”© ìˆ˜í–‰
        try:
            embedding = get_text_embedding(combined_text, self.genai)
            return embedding, None, PageStatus.SUCCESS
        except Exception as e:
            return None, f"ì„ë² ë”© ì˜¤ë¥˜: {e}", PageStatus.FAILED
    

    def invoke_indexing(self, page: PDFPage) -> Tuple[str, str, PageStatus, Optional[str]]:
        """
        ELSì— í˜ì´ì§€ ì¸ë±ì‹± ìˆ˜í–‰
        """
        try:
            data = {
                "page_id": page.page_id,
                "doc_id": page.doc_id,
                "page_number": page.page_number,
                "gcs_path": page.gcs_path,
                "gcs_pdf_path": page.gcs_pdf_path,
                "extracted_text": page.extracted_text,
                "summary": page.summary,
                "embedding": json.loads(page.embedding),
            }

            self.els.conn.index(index=INDEX_NAME, id=page.page_id, document=data)
            return page.page_id, page.gcs_path, PageStatus.SUCCESS, None

        except Exception as e:
            return page.page_id, page.gcs_path, PageStatus.FAILED, f"Indexing Error: {e}"

------------------

## processor/embedder.py

import os
import sys
from typing import Tuple, Optional, List

from google import genai
from google.genai.types import EmbedContentConfig

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from config import EMBEDDING_MODEL


def get_text_embedding(text: str, client: genai.Client) -> List[float]:
    response = client.models.embed_content(
        model=EMBEDDING_MODEL,
        contents=[text],
        config=EmbedContentConfig(
            task_type="RETRIEVAL_DOCUMENT",
            output_dimensionality=768,
            title="Content of Document"
        )
    )
    return response.embeddings[0].values


if __name__ == "__main__":
    ############### í•¨ìˆ˜ ë™ì‘ í…ŒìŠ¤íŠ¸ ###############
    from config import PROJECT_ID, GENAI_LOCATION
    genai_client = genai.Client(
        vertexai=True,
        project=PROJECT_ID,
        location=GENAI_LOCATION,
    )

    res = get_text_embedding(text="ì•ˆë…•í•˜ì„¸ìš”", client=genai_client)
    print(f"length: {len(res)}")
    print(res[:5])

------------------

## scheduler/orchestrator.py

import os
import sys
import json
import time
from typing import List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

from google.cloud import storage
from google import genai

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from db.initialize import initialize_tables
from db.models import PageStatus
from db.session import get_db_session
from db.repository import Repository
from storage.gcs_client import GCSStorageClient
from processor.pdf_manager import PDFManager
from processor.elastic import ESConnector
from utils.logger import get_logger
from utils.utils import compute_doc_hash
from config import (
    GCS_SOURCE_BUCKET,
    GCS_PROCESSED_BUCKET,
    PROJECT_ID, 
    GENAI_LOCATION,
    ES_HOST,
    ES_USER,
    ES_PWD,
    INDEX_NAME,
    LOG_LEVEL
)

logger = get_logger(__name__, LOG_LEVEL)

def run_pipeline() -> None:

    logger.info("[Step 0] Initializing database tables")
    initialize_tables()


    db_gen = get_db_session()
    session = next(db_gen)

    try:
        # â”€â”€ ì´ˆê¸°í™”
        gcs_client = storage.Client()
        storage_client = GCSStorageClient(GCS_SOURCE_BUCKET, GCS_PROCESSED_BUCKET, gcs_client)
        repo = Repository(session)
        genai_client = genai.Client(vertexai=True, project=PROJECT_ID, location=GENAI_LOCATION)
        els = ESConnector(hosts=ES_HOST, credentials=(ES_USER, ES_PWD))

        manager = PDFManager(storage_client, repo, genai_client, els)


        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 1. ì‹ ê·œë¬¸ì„œ Detection
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info("[Step 1] Scanning GCS for new PDF documents")
        pdf_paths = storage_client.list_pdfs()
        logger.info(" â””â”€â”€ Found %d PDF files in GCS", len(pdf_paths))
        
        known_doc_ids = repo.list_all_document_hashes()
        new_docs: List[Tuple[str, str]] = []

        for path in pdf_paths:
            try:
                doc_hash = compute_doc_hash(storage_client, path)
                if doc_hash not in known_doc_ids:
                    new_docs.append((doc_hash, path))
            except Exception as e:
                logger.warning(" â””â”€â”€ Hash computation failed for %s (%s)", path, e)

        logger.info(" â””â”€â”€ Detected %d new documents", len(new_docs))


        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 2. ì‹ ê·œë¬¸ì„œ Splití•´ì„œ DB ë“±ë¡
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info("[Step 2] Splitting new documents and saving page metadata")

        # ë³‘ë ¬ì²˜ë¦¬ ë¯¸ì ìš©
        for i, (doc_id, gcs_pdf_path) in enumerate(new_docs):
            try:
                # PDFDocument Table ë“±ë¡
                if not repo.exists_document(doc_id):
                    repo.create_document(doc_id, gcs_pdf_path)

                # PDFPage Table ë“±ë¡
                gcs_page_infos = manager.invoke_split(gcs_pdf_path) # {page_number: gcs_image_path}                
                for page_number, gcs_image_path in gcs_page_infos.items():
                    repo.create_page_record(
                        doc_id=doc_id,
                        page_number=page_number,
                        gcs_path=gcs_image_path,
                        gcs_pdf_path=gcs_pdf_path,
                    )  

                logger.info(" â””â”€â”€ [%d/%d] Split and saved %d pages: %s", i + 1, len(new_docs), len(gcs_page_infos), gcs_pdf_path)

            except Exception as e:
                logger.warning(" â””â”€â”€ [%d/%d] Failed to split or save: %s (%s)", i + 1, len(new_docs), gcs_pdf_path, e)


        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 3. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info("[Step 3] Extracting text from page images")
        extraction_pages = repo.get_pages_for_extraction()
        pending = [p for p in extraction_pages if p.extracted == PageStatus.PENDING]
        retry   = [p for p in extraction_pages if p.extracted == PageStatus.FAILED]
        logger.info("Pages queued for text extraction: %d (new: %d, retry: %d)", len(extraction_pages), len(pending), len(retry))

        # ë³‘ë ¬ì²˜ë¦¬ ë¯¸ì ìš©
        for i, page in enumerate(extraction_pages, 1):
            try:
                tag = "new" if page.extracted == PageStatus.PENDING else "retry"
                text, error, status = manager.invoke_extraction(page.gcs_path)

                repo.update_page_record(
                    page_id=page.page_id,
                    extracted_text=text,
                    extracted=status,
                    error_message=error
                )   

                if status == PageStatus.SUCCESS:
                    logger.debug(" â””â”€â”€ [%d/%d] Text extraction succeeded (%s): %s", i, len(extraction_pages), tag, page.gcs_path)
                else:
                    logger.warning(" â””â”€â”€ [%d/%d] Text extraction failed (%s): %s - %s", i, len(extraction_pages), tag, page.gcs_path, error)
                    time.sleep(60)

            except Exception as e:
                logger.error(" â””â”€â”€ [%d/%d] Text extraction exception (%s): %s - %s", i, len(extraction_pages), tag, page.gcs_path, e)


        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 4. ìš”ì•½ ì¶”ì¶œ
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info("[Step 4] Generating summaries")
        summary_pages = repo.get_pages_for_summary()
        pending = [p for p in summary_pages if p.summarized == PageStatus.PENDING]
        retry   = [p for p in summary_pages if p.summarized == PageStatus.FAILED]
        logger.info("Pages queued for summary generation: %d (new: %d, retry: %d)", len(summary_pages), len(pending), len(retry))

        # ë³‘ë ¬ì²˜ë¦¬ ë¯¸ì ìš©
        for i, page in enumerate(summary_pages, 1):
            try:
                tag = "new" if page.summarized == PageStatus.PENDING else "retry"
                summary, error, status = manager.invoke_summary(page.gcs_path)

                repo.update_page_record(
                    page_id=page.page_id,
                    summary=summary,
                    summarized=status,
                    error_message=error
                )

                if status == PageStatus.SUCCESS:
                    logger.debug(" â””â”€â”€ [%d/%d] Summary generation succeeded (%s): %s", i, len(summary_pages), tag, page.gcs_path)
                else:
                    logger.warning(" â””â”€â”€ [%d/%d] Summary generation failed (%s): %s - %s", i, len(summary_pages), tag, page.gcs_path, error)
                    time.sleep(60)

            except Exception as e:
                logger.error(" â””â”€â”€ [%d/%d] Summary generation exception (%s): %s - %s", i, len(summary_pages), tag, page.gcs_path, e)



        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 5. ì„ë² ë”© ë²¡í„° ìƒì„±
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info("[Step 5] Generating embeddings")
        embedding_pages = repo.get_pages_for_embedding()
        pending = [p for p in embedding_pages if p.embedded == PageStatus.PENDING]
        retry   = [p for p in embedding_pages if p.embedded == PageStatus.FAILED]
        logger.info("Pages queued for embedding: %d (new: %d, retry: %d)", len(embedding_pages), len(pending), len(retry))

        # ë³‘ë ¬ì²˜ë¦¬ ë¯¸ì ìš©
        for i, page in enumerate(embedding_pages, 1):
            try:
                tag = "new" if page.embedded == PageStatus.PENDING else "retry"
                embedding, error, status = manager.invoke_embedding(page.gcs_path)

                repo.update_page_record(
                    page_id=page.page_id,
                    embedding=json.dumps(embedding), # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                    embedded=status,
                    error_message=error
                )

                if status == PageStatus.SUCCESS:
                    logger.debug(" â””â”€â”€ [%d/%d] Embedding succeeded (%s): %s", i, len(embedding_pages), tag, page.gcs_path)
                else:
                    logger.warning(" â””â”€â”€ [%d/%d] Embedding failed (%s): %s - %s", i, len(embedding_pages), tag, page.gcs_path, error)
                    time.sleep(60)

            except Exception as e:
                logger.error(" â””â”€â”€ [%d/%d] Embedding exception (%s): %s - %s", i, len(embedding_pages), tag, page.gcs_path, e)



        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 6. ì¸ë±ì‹±
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info("[Step 6] Indexing")
        indexing_pages = repo.get_pages_for_indexing()
        pending = [p for p in embedding_pages if p.indexed == PageStatus.PENDING]
        retry   = [p for p in embedding_pages if p.indexed == PageStatus.FAILED]
        logger.info("Pages queued for indexing: %d (new: %d, retry: %d)", len(embedding_pages), len(pending), len(retry))

        # ë³‘ë ¬ì²˜ë¦¬ ë¯¸ì ìš©
        for i, page in enumerate(indexing_pages, 1):
            try:
                tag = "new" if page.indexed == PageStatus.PENDING else "retry"
                page_id, gcs_page_path, status, error = manager.invoke_indexing(page)

                repo.update_page_record(
                    page_id=page_id,
                    indexed=status,
                    error_message=error
                )

                if status == PageStatus.SUCCESS:
                    logger.debug(" â””â”€â”€ [%d/%d] Indexing succeeded (%s): %s", i, len(indexing_pages), tag, page.gcs_path)
                else:
                    logger.warning(" â””â”€â”€ [%d/%d] Indexing failed (%s): %s - %s", i, len(indexing_pages), tag, page.gcs_path, error)

            except Exception as e:
                logger.error(" â””â”€â”€ [%d/%d] Indexing exception (%s): %s - %s", i, len(indexing_pages), tag, page.gcs_path, e)


    finally:
        session.close()
        logger.info("\nPipeline execution finished")

------------------

## storage/gcs_client.py

from typing import List
from google.cloud import storage

class GCSStorageClient:
    def __init__(self, source_bucket: str, target_bucket: str, client: storage.Client) -> None:
        self.source_bucket: str = source_bucket
        self.target_bucket: str = target_bucket
        self.client: storage.Client = client
    
    def list_pdfs(self, prefix: str = "") -> List[str]:
        bucket = self.client.bucket(self.source_bucket)
        blobs = bucket.list_blobs(prefix=prefix)
        return [blob.name for blob in blobs if blob.name.lower().endswith(".pdf")]
    
    def download_file(self, gcs_path: str, local_path: str, bucket_name: str) -> str:
        bucket = self.client.bucket(bucket_name)
        blob = bucket.blob(gcs_path)
        blob.download_to_filename(local_path)
        return local_path
    
    def upload_file(self, local_image_path: str, gcs_path: str, bucket_name: str) -> str:
        bucket = self.client.bucket(bucket_name)
        blob = bucket.blob(gcs_path)
        blob.upload_from_filename(local_image_path)
        return gcs_path
    
    def make_output_path(self, src_gcs_path: str, page_num: int) -> str:
        parts = src_gcs_path.rsplit("/", 1)
        dirs = parts[0] if len(parts) > 1 else ""
        filename = parts[-1]
        base = filename.rsplit(".", 1)[0]
        folder = f"{dirs}/{base}" if dirs else base
        page_fname = f"{base}-page-{page_num:05d}.png"
        return f"{folder}/{page_fname}"
    

------------------

## utils/utils.py

import os
import sys
import hashlib

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from storage.gcs_client import GCSStorageClient


def get_file_hash(file_path, hash_type='sha256'):
    # ì‚¬ìš©í•  í•´ì‹œ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
    hash_func = getattr(hashlib, hash_type)()

    # íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ ì½ìœ¼ë©´ì„œ í•´ì‹œ ì—…ë°ì´íŠ¸
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_func.update(chunk)

    return hash_func.hexdigest()


def compute_doc_hash(storage_client: GCSStorageClient, gcs_pdf_path: str) -> str:
    """Download a PDF to a tmp file and return its sha-256 hash."""
    import tempfile
    with tempfile.NamedTemporaryFile(suffix=".pdf") as tmp:
        storage_client.download_file(gcs_pdf_path, tmp.name, storage_client.source_bucket)
        return get_file_hash(tmp.name)

def split_file_path(path: str):
    """
    ê²½ë¡œ ë¶„ë¦¬: "folder/subfolder/doc.pdf" â†’ "folder/subfolder", "doc.pdf"
    """
    if "/" in path:
        parent_dir, filename = path.rsplit("/", 1)
    else:
        parent_dir, filename = "", path
    return parent_dir, filename




# --- Git Utils
import os
from pathspec import PathSpec
from pathspec.patterns import GitWildMatchPattern

def load_gitignore(path=".gitignore"):
    if not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8") as f:
        patterns = f.read().splitlines()
    return PathSpec.from_lines(GitWildMatchPattern, patterns)

def generate_tree(dir_path, prefix="", spec=None, base_path=""):
    tree_str = ""
    entries = sorted(os.listdir(dir_path))
    entries = [e for e in entries if not e.startswith('.')]  # ìˆ¨ê¹€ íŒŒì¼ ê¸°ë³¸ ì œê±°

    for index, entry in enumerate(entries):
        full_path = os.path.join(dir_path, entry)
        relative_path = os.path.relpath(full_path, base_path)

        if spec and spec.match_file(relative_path):
            continue  # .gitignoreì— í•´ë‹¹ë˜ë©´ ìŠ¤í‚µ

        connector = "â””â”€â”€ " if index == len(entries) - 1 else "â”œâ”€â”€ "
        tree_str += f"{prefix}{connector}{entry}\n"

        if os.path.isdir(full_path):
            extension = "    " if index == len(entries) - 1 else "â”‚   "
            tree_str += generate_tree(full_path, prefix + extension, spec, base_path)
    return tree_str

def save_to_readme(tree_str, readme_path="README.md"):
    with open(readme_path, "a", encoding="utf-8") as f:
        f.write("\n```\n")
        f.write(tree_str)
        f.write("```\n")

if __name__ == "__main__":
    base_dir = "."
    gitignore_spec = load_gitignore()
    tree_output = generate_tree(base_dir, spec=gitignore_spec, base_path=base_dir)
    save_to_readme(tree_output)
    print("ğŸ“ .gitignore ì œì™¸í•˜ê³  README.mdì— êµ¬ì¡°ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")

------------------

## utils/logger.py

import logging
import sys
from typing import Optional

# _FMT = "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s"
_FMT = "%(asctime)s | %(levelname)-8s | %(message)s"


def get_logger(name: str,
               level: str | int = "INFO",
               stream: Optional[object] = None) -> logging.Logger:
    logger = logging.getLogger(name)
    # if already configured â€“ return asâ€‘is
    if logger.handlers:
        return logger

    handler = logging.StreamHandler(stream or sys.stdout)
    handler.setFormatter(logging.Formatter(_FMT))
    logger.addHandler(handler)
    logger.setLevel(level)
    logger.propagate = False  # stop doubleâ€‘logs
    return logger

------------------

## db/initialize.py

import os
import sys
from sqlalchemy import inspect

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from db.models import Base
from db.session import engine
from utils.logger import get_logger
from config import LOG_LEVEL

logger = get_logger(__name__, LOG_LEVEL)


def print_table_infos():
    inspector = inspect(engine)
    tables = inspector.get_table_names()

    print("\ní˜„ì¬ DB í…Œì´ë¸” ë° ì»¬ëŸ¼ ì •ë³´:")
    for table in tables:
        print(f"\nTABLE NAME: {table}")
        columns = inspector.get_columns(table)
        for col in columns:
            name = col['name']
            type_ = col['type']
            nullable = col['nullable']
            default = col.get('default', None)
            print(f"  - {name} ({type_}){' NULL' if nullable else ' NOT NULL'}"
                  f"{' DEFAULT ' + str(default) if default is not None else ''}")
    return tables

def initialize_tables():
    inspector = inspect(engine)
    existing_tables = inspector.get_table_names()

    # ìµœì†Œí•œì˜ ê¸°ì¤€ í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ íŒë‹¨
    if "pdf_documents" not in existing_tables or "pdf_pages" not in existing_tables:
        logger.info(" â”Œâ”€â”€ There is no tables â†’ Try to create tables")
        Base.metadata.create_all(bind=engine)
        logger.info(" â””â”€â”€ Created tables!")
    else:
        logger.info(" â””â”€â”€ Already exists all tables")
    
    # print_table_infos()


if __name__ == "__main__":
   initialize_tables()

------------------

## db/repository.py

import os
import sys
from typing import List

from sqlalchemy import distinct
from sqlalchemy.orm import Session
from sqlalchemy import select

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from db.models import PDFDocument, PDFPage, PageStatus
from storage.gcs_client import GCSStorageClient
from utils.logger import get_logger
from config import LOG_LEVEL

class Repository:
    def __init__(self, session: Session) -> None:
        self.session = session
        self.logger = get_logger(self.__class__.__name__, LOG_LEVEL)


    def list_all_document_hashes(self) -> set[str]:
        result = (
            self.session.query(PDFDocument.doc_id)
            .all()
        )
        return {row[0] for row in result}


    def exists_document(self, doc_id: str) -> bool:
        return self.session.query(PDFDocument).filter_by(doc_id=doc_id).first() is not None


    def create_document(self, doc_id: str, gcs_path: str) -> PDFDocument:
        doc = PDFDocument(doc_id=doc_id, gcs_path=gcs_path)
        self.session.add(doc)
        self.session.commit()
        return doc


    def create_page_record(self, doc_id: str, page_number:int, gcs_path: str, gcs_pdf_path: str) -> PDFPage:
        page = PDFPage(
            page_id=f"{doc_id}_{page_number:05d}",
            doc_id=doc_id,
            page_number=f"{page_number:05d}",
            gcs_path=gcs_path,
            gcs_pdf_path=gcs_pdf_path
        )
        self.session.add(page)
        self.session.commit()
        return page


    def update_page_record(self, page_id: str, **kwargs):
        page = self.session.get(PDFPage, page_id)
        if page:
            for k, v in kwargs.items():
                if v is not None:
                    setattr(page, k, v)
            self.session.commit()


    def get_first_n_pages(self, doc_id: str, n: int = 5) -> List[str]:
        stmt = (
            select(PDFPage.gcs_path)
            .where(PDFPage.doc_id == doc_id)
            .order_by(PDFPage.page_number.asc())
            .limit(n)
        )
        return self.session.scalars(stmt).all()


    def get_pages_for_extraction(self) -> List[PDFPage]:
        return (
            self.session.query(PDFPage)
            .filter(PDFPage.extracted.in_([PageStatus.PENDING, PageStatus.FAILED]))
            .all()
        )


    def get_pages_for_summary(self) -> List[PDFPage]:
        return (
            self.session.query(PDFPage)
            .filter(PDFPage.summarized.in_([PageStatus.PENDING, PageStatus.FAILED]))
            .all()
        )

    def get_pages_for_embedding(self) -> List[PDFPage]:
        return (
            self.session.query(PDFPage)
            .filter(PDFPage.extracted == PageStatus.SUCCESS)
            .filter(PDFPage.summarized == PageStatus.SUCCESS)
            .filter(PDFPage.embedded.in_([PageStatus.PENDING, PageStatus.FAILED]))
            .all()
        )


    def get_pages_for_indexing(self) -> List[PDFPage]:
        return (
            self.session.query(PDFPage)
            .filter(PDFPage.embedded == PageStatus.SUCCESS)
            .filter(PDFPage.indexed.in_([PageStatus.PENDING, PageStatus.FAILED]))
            .all()
        )

------------------

## db/models.py

import os
import sys
import enum
from datetime import datetime

from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from sqlalchemy import (
    Column, Integer, String, Text, DateTime, Enum, ForeignKey
)
from sqlalchemy.dialects.mysql import LONGTEXT
from sqlalchemy.sql import func

PROJECT_PATH = os.path.dirname(os.path.dirname(__file__))
sys.path.append(PROJECT_PATH)

from config import TABLENAME_PDFPAGE, TABLENAME_PDFDOCUMENT


# ëª¨ë“  ORM ëª¨ë¸ì˜ ê¸°ë³¸ì´ ë˜ëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜
# ì•„ë˜ì˜ í…Œì´ë¸” í´ë˜ìŠ¤ë“¤ì´ ì´ Baseë¥¼ ìƒì†í•´ì„œ ë§Œë“¤ì–´ì•¼ ì‹¤ì œ í…Œì´ë¸”ë¡œ ì¸ì‹ë¨
Base = declarative_base()

class PageStatus(enum.Enum):
    PENDING = "PENDING"     # ì•„ì§ ì‹œë„ ì•ˆ í•¨
    SUCCESS = "SUCCESS"     # ì²˜ë¦¬ ì™„ë£Œ
    FAILED = "FAILED"       # ì²˜ë¦¬ ì‹¤íŒ¨

class PDFPage(Base):
    __tablename__ = TABLENAME_PDFPAGE

    page_id: int = Column(String(128), primary_key=True)
    doc_id: str = Column(String(128), ForeignKey("pdf_documents.doc_id"), nullable=False)
    page_number: str = Column(String(32), nullable=False)
    
    gcs_path: str = Column(String(1000), nullable=False)
    gcs_pdf_path: str = Column(String(1000), nullable=False)
    
    extracted_text: str = Column(LONGTEXT, nullable=True)
    summary: str = Column(LONGTEXT, nullable=True)
    embedding: str = Column(LONGTEXT, nullable=True)
    
    extracted: PageStatus = Column(Enum(PageStatus), default=PageStatus.PENDING)
    summarized: PageStatus = Column(Enum(PageStatus), default=PageStatus.PENDING)
    embedded: PageStatus = Column(Enum(PageStatus), default=PageStatus.PENDING)
    indexed: PageStatus = Column(Enum(PageStatus), default=PageStatus.PENDING)

    error_message: str = Column(Text, nullable=True)
    created_at: datetime = Column(DateTime, default=datetime.utcnow)
    updated_at: datetime = Column(DateTime, default=func.now(), onupdate=func.now())

    document = relationship("PDFDocument", back_populates="pages")


class PDFDocument(Base):
    __tablename__ = TABLENAME_PDFDOCUMENT

    doc_id: str = Column(String(128), primary_key=True)
    gcs_path: str = Column(String(1000), nullable=False)
    created_at: datetime = Column(DateTime, default=datetime.utcnow)

    pages = relationship("PDFPage", back_populates="document", cascade="all, delete-orphan")
        # back_populates="document"     : document.pagesë¡œ í˜ì´ì§€ ì ‘ê·¼ ê°€ëŠ¥, page.documentë¡œ í•´ë‹¹ í˜ì´ì§€ê°€ ì†í•œ ë¬¸ì„œ í™•ì¸ê°€ëŠ¥
        # cascade="all, delete-orphan"  : ë¶€ëª¨(PDFDocument)ê°€ ì‚­ì œë˜ì—ˆì„ë•Œ ìì‹(PDFPage)ë„ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë˜ë„ë¡

------------------

## db/session.py

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session
from typing import Generator
from config import (
    MYSQL_USER, MYSQL_PWD, MYSQL_HOST,
    MYSQL_PORT, MYSQL_DB, MYSQL_CHARSET
)

# SQLAlchemy + PyMySQL ì—°ê²° ë¬¸ìì—´ ìƒì„±
# DATABASE_URL: str = (
#     f"mysql+pymysql://{MYSQL_USER}:{MYSQL_PWD}"
#     f"@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}"
#     f"?charset={MYSQL_CHARSET}"
# )
DATABASE_URL = "mysql+pymysql://hde_chat:dpdlcldecot1%40@10.100.79.63:23306/hde_chat?charset=utf8mb4"


# ì—”ì§„ ìƒì„± (pool_pre_ping=True ë¡œ ì—°ê²° ëŠê¹€ ë°©ì§€)
engine = create_engine(
    DATABASE_URL,
    echo=False,
    pool_pre_ping=True
)

# ì„¸ì…˜ íŒ©í† ë¦¬
SessionLocal: sessionmaker = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine
)

def get_db_session() -> Generator[Session, None, None]:
    """
    Dependency ë˜ëŠ” ì§ì ‘ í˜¸ì¶œìš©ìœ¼ë¡œ,
    yield í›„ ë°˜ë“œì‹œ .close() ë˜ë„ë¡ ì„¤ê³„.
    """
    session: Session = SessionLocal()
    try:
        yield session
    finally:
        session.close()

------------------
`